---
title: "Continuous Variables"
subtitle: "Measures and Basic Tests"
author: "Frances Hung"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(tidyverse)
library(Gmisc)
library(Hmisc)
library(gtsummary)
library(broom)
library(medicaldata)
library(PASWR2)
#library(flextable)
```

Today, we will briefly go over common measures used to describe continuous data:

-   Centrality measures (mean, median, mode)
-   Measures of spread (standard deviation, range, interquartile range)

Statistical models will often make inferences on centrality measures, and these models will also need measures of spread as input. 

After this, we will cover basic ways of comparing continuous measures between groups differentiated by a characteristic. Some of these concepts (t-tests, correlation) are also applicable to and important for understanding discrete data as well.

## Review: Data Cleaning and Visualization

Since today's lecture is focused on continuous variables, let's clean and visualize a dataset that we can use for the remainder of the lecture. For this lecture and the next few, we'll use a dataset called `indo_rct` which has data from a randomized control trial (RCT) on using indomethacin for prevention of post-ERCP pancreatitis. 

For today's outcome, we'll focus on the continuous measure `risk` (a patient's estimated risk score). Other variables that we'll need for today are:

- age
- pep (risk factor: previous post-ERCP pancreatitis)
- psphinc (risk factor: whether pancreatic sphincteromy was performed)

We start by getting a general sense of the data. Let's take a peek.

```{r head-indo, options}
indo_rct_day3 <- indo_rct %>%
  select(risk, age, pep, psphinc)

head(indo_rct_day3)

# the categorical variables are already in factor form, so no need to modify inside summary command
summary(indo_rct_day3)
```
Notice that all categorical variables have a number, underscore, no/yes for a value. This is redundant information, so we can get rid of the number and underscore.

```{r clean, options}
indo_rct_day3_clean <- indo_rct_day3 %>%
  mutate(across(c(pep, psphinc),
                ~factor(str_remove(.x, "(.*)_"))))

head(indo_rct_day3_clean)
```


## Centrality Measures

The goal of statistics is to simplify data into easily interpretable numbers. The ones we often use to describe "average" behavior include the mean, median, and mode.

The **mean** of a continuous variable is the sum of continuous values from the sample divided by the sample size. 

The **median** of a continuous variable is the number at the 

The **mode** of a continuous variable may not be as meaningful as the other two centrality measures.

To compare centrality measures by group, we calculate the mean and median of risk score by pep and psphinc using `group_by` and `summarize` from tidyverse.

```{r centrality-ex, options}
indo_rct_day3_clean %>%
  group_by(pep, psphinc) %>%
  summarise(mean_risk = mean(risk),
            median_risk = median(risk))
```

## Group comparisons of events

We can talk about basic ways of comparing continuous centrality measures among different groups, now that we've defined what centrality and spread measures are. This is our first foray into statistical tests. This workshop is not meant to give statistical background on these tests, so stay tuned for the in-person workshop (or consult with a statistician) before using these tests in research projects.

Below are three different ways of describing or testing continuous centrality measure differences among groups.

-   Correlation
-   T-test
-   Wilcoxon (here for reference, will not cover)

More sophisticated ways of testing continuous measure differences between groups (like linear regression, adjusting for other variables) will be introduced later.

## Correlation

Correlation is a rough measure of how associated two variables are. Most correlation measures range from -1 to 1, where a correlation of 1 means as one variable becomes more positive, the other does as well. A correlation of -1 means that as one variable becomes more positive, the other becomes more negative, and a correlation of 0 means no relationship.

The code below calculates the Pearson correlation measure, which assumes a linear relationship between continuous variables, as well as the Spearman correlation, which doesn't.

We can use correlation to look at event-related variables as well, as we'll see in our next lecture.

```{r cor, options}
cor(indo_rct_day3_clean$risk, indo_rct_day3_clean$age,
    method = "pearson")

cor(indo_rct_day3_clean$risk, indo_rct_day3_clean$age,
    method = "spearman")
```

## 2-Group Comparisons

### T-test (Parametric)

The t-test approximates how likely we'd see the observed difference in means, assuming that the true means of the groups are actually the same. A small p-value means that there is evidence that the true means of the groups are different.

It assumes that if we took many samples from the two group populations, the resulting sample means would be normally distributed (bell-shaped). This usually holds if we have a large enough sample size. It also assumes the samples are randomly sampled and independent from one another.

There are several variations of t-test, depending on what assumptions we make. Today, we go over the case where the two groups have unknown, different population variances (most common).

We can either supply the raw tidy data to a t-test function, or we can supply the means, standard deviations, and counts of each group sample to a summarized t-test function.

#### From Raw Data

```{r raw-t-test, options}
t.test(indo_rct_day3_clean$risk ~ indo_rct_day3_clean$pep, #alternative hypothesis: mean risk differs by pep status
       alternative = "two.sided", #two-sided alternative hypothesis
       mu = 0, #null hypothesis: difference is 0
       paired = FALSE, #group observations are not paired 
       var.equal = FALSE, #variances are not equal
       conf.level = 0.95) #alpha (type-I error) is 0.05
```

#### From Means and Sample Variances

```{r paswr-t-test, options}

#--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
# find mean and SD risk by pep group
#--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
sumStatsPepRisk <- indo_rct_day3_clean %>%
  group_by(pep) %>%
  summarise(meanRisk = mean(risk),
            sdRisk = sd(risk),
            countRisk = n())


#--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
# input mean and SD by pep group into summarized t-test
#--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
tsum.test(mean.x = sumStatsPepRisk %>% 
            filter(pep == "yes") %>%
            .$meanRisk,
          s.x = sumStatsPepRisk %>% 
            filter(pep == "yes") %>%
            .$sdRisk,
          n.x = sumStatsPepRisk %>% 
            filter(pep == "yes") %>%
            .$countRisk,
          mean.y = sumStatsPepRisk %>% 
            filter(pep == "no") %>%
            .$meanRisk,
          s.y = sumStatsPepRisk %>% 
            filter(pep == "no") %>%
            .$sdRisk,
          n.y = sumStatsPepRisk %>% 
            filter(pep == "no") %>%
            .$countRisk,
          alternative = "two.sided", #two-sided alternative hypothesis
          mu = 0, #null hypothesis: difference is 0
          var.equal = FALSE, #variances are not equal
          conf.level = 0.95) #alpha (type-I error) is 0.05
```

#### Changing Parameters

In both the t-test and summarized t-test functions, we can change parameter values to suit our problem of interest:

- alternative: we can specify this to be "less" or "greater" if our alternative hypothesis is one-sided
- conf.level: if our chosen alpha type-I level is something other than the default 0.05 (conf.level=0.95), we specify here
- paired: if we want to use a paired t-test, we set this to TRUE

### Wilcoxon Rank Sum (Non-parametric)

This test will not be covered in the in-person workshop and we will not go over it during lecture, but it will be here for reference since it is often used.

Similarly to the t-test, the chi-squared test approximates how likely we'd see the observed difference in proportions, assuming that the true **medians** of the groups are actually the same. A small p-value means that there is evidence that the true medians of the groups are different.

It doesn't have the same normality assumption as the t-test, making it good for small samples where the assumption may not hold. However, the t-test is more powerful (has a higher ability to correctly reject the null hypothesis if the alternative is in fact true) than the Wilcoxon under correct normality assumptions. The mean rather than the median is often of more interest to researchers, since it is what is available for interpretation when we do standard regression.

```{r wilcoxon, options}
wilcox.test(indo_rct_day3_clean$risk ~ indo_rct_day3_clean$pep)
```

## ANOVA: Comparisons of more than 2 groups

The above measures (t-test, Wilcoxon Rank Sum) test if there's evidence that a continuous variable differs between two groups. In order to test if a continuous variable differs between more than 2 groups, we can use ANOVA.

Like the t-test, ANOVA assumes that the continuous values in each group are roughly normally distributed, randomly sampled, and independent.

```{r dataf, options}
hiv_rna_sim <- data.frame(hospital=c(rep("A", 20), rep("B", 20), rep("C", 20)),
           treatment=rep(c(rep("trt", 10), rep("control", 10)), 3))

anova_df <- hiv_rna_sim %>%
  mutate(hospMean = case_when(hospital=="A" ~ 10,
                              hospital=="B" ~ 12,
                              hospital=="C" ~ 24),
         trtEffect = if_else(treatment == "trt", 12, 0)) %>%
  rowwise() %>%
  mutate(hiv_rna = hospMean + rnorm(1, 0, 7) + 
           trtEffect + rnorm(1, 0, 5))

```

### One-way ANOVA

To run one-way ANOVA (whether the continuous quantity differs by one factor), use the `aov` command. The `aov` command takes in two arguments. The first is a formula-type object, with the dependent continuous variable on the left and the independent categorical variable on the right of the `~`. The second is the tidy dataset.

An equivalent way of using the command is omitting the data argument and using vectors in the first argument (see below, commented out).

To ensure that ANOVA treats the independent variable as a categorical variable, we designate all character variables as factors either in the formula or before running, in the dataset.

```{r aov, options}
anova.risk <- aov(hiv_rna ~ factor(hospital),
    data = anova_df)

# equivalent
# anova.risk <- aov(anova_df$hiv_rna ~ factor(anova_df$hospital))

```

To see results, we use the `summary` command. The key value to look at here is the `Pr(>F)` column; if it is less than our chosen type-I error (in most cases, 0.05), then there is evidence that at least one pair of groups has differing continuous variable means. 

```{r summary-aov, options}
summary(anova.risk)
```

In this example, the F-value is less than 0.05, so we conclude that there is evidence that at least one hospital has different mean HIV RNA measurements than another.

### Tukey HSD

In order to determine which hospitals have different mean HIV RNA measurements, we need to use an additional test, accounting for multiple comparisons between pairs of groups.

We can use the Tukey HSD test. The `TukeyHSD` command takes in one argument: the anova fit object that we want to run the test on.

```{r tukey, options}
TukeyHSD(anova.risk)
```
The `p adj` column indicates the p-values adjusted for multiple comparisons for each pair of hospitals. Looking for adjusted p-values less than 0.05, we can see that hospital C has significantly different means from hospitals A and B. 

### Two-way ANOVA

We can account for an additional variable in ANOVA via two-way ANOVA. We can add the `treatment` variable to the formula argument in `aov`.

```{r aov-2, options}
anova.risk <- aov(hiv_rna ~ hospital + treatment,
    data = anova_df)

summary(anova.risk)
```

The summary of our model fit tells us there are differences in mean HIV RNA between treatments and at least one pair of hospitals.

We can use Tukey HSD as previously demonstrated for more information on which hospitals differ in mean HIV RNA.

### Other Extensions

More complex ANOVA models can be fit by changing the formula argument in `aov`. For repeated measures ANOVA, for instance, we are often interested in looking at differences across different times (each subject has observations at these times). Let's say we have a dataframe with columns `subject`, `timepoint`, and `hiv_rna`, tracking RNA levels at various times for each subject. To compare RNA levels across time, a starting ANOVA model formula would be something like

$$hiv\_rna \sim factor(timepoint)$$

To account for additional "noise" added by each subject for each timepoint, we add an error term `Error(subject/timepoint)`.

$$hiv\_rna \sim factor(timepoint) + Error(subject/timepoint)$$

# Exercises

We'll see if mean or median age is different between pep groups.

1. Use `group_by` and `summarise` to find the mean and median age for patients with and without pep.
2. Use a two-sided t-test to test if mean age differs between patients with and without pep.
3. Use a one-sided t-test to test if mean age is lower in patients with compared to patients without pep.
4. Use two-way ANOVA to test if mean age differs in patients with/without pep and patients with/without psphinc. Interpret the results.
5. Look up how to add interaction effects into the formula argument for `aov`. Add an interaction between pep and psphinc to the two-way ANOVA model. Use TukeyHSD to help interpretation.

```{r answers-comparison-tests, options}
t.test(indo_rct_day3_clean$age ~ indo_rct_day3_clean$pep, #alternative hypothesis: mean risk differs by pep status
       alternative = "two.sided", #two-sided alternative hypothesis
       mu = 0, #null hypothesis: difference is 0
       paired = FALSE, #group observations are not paired 
       var.equal = FALSE, #variances are not equal
       conf.level = 0.95) #alpha (type-I error) is 0.05

t.test(indo_rct_day3_clean$age ~ indo_rct_day3_clean$pep, #alternative hypothesis: mean risk differs by pep status
       alternative = "greater", #two-sided alternative hypothesis
       mu = 0, #null hypothesis: difference is 0
       paired = FALSE, #group observations are not paired 
       var.equal = FALSE, #variances are not equal
       conf.level = 0.95) #alpha (type-I error) is 0.05

anova.age <- aov(age ~ pep + psphinc,
    data = indo_rct_day3_clean)
summary(anova.age)

anova.age.interact <- aov(age ~ pep + psphinc + pep:psphinc,
    data = indo_rct_day3_clean)
summary(anova.age.interact)
TukeyHSD(anova.age.interact)
```
