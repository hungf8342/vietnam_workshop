---
title: "Regression and Survival"
subtitle: "A Brief Overview"
author: "Frances Hung"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)

```

Today, we will go over a very brief overview on regression models and survival models. So far, we've focused on univariate (taking into account a single variable) comparisons. For example, in a t-test, we look only at whether a mean outcome differs between two groups.

Regression models allow us to take into account multiple variables when modeling outcomes. This is especially important in analyzing observational data, where main groups of interest may have different characteristics that need to be accounted for.


# Packages Used Today

-   base R (loaded by default and used for any coding in R)
- `tidyverse` for data wrangling
- `broom` for tidying up regression summary output
- `medicaldata` and `ROCit` the indomethacin and diabetes datasets
- `survival` and `survminer` for survival models
- `kableExtra` for making dataframes into tables

```{r libs, options}
library(tidyverse)
library(broom)
library(medicaldata)
library(survival)
library(survminer)
library(kableExtra)
library(ROCit)
```


# Review: Univariate T-Test

We will start today's lecture with linear regression, so we will first go through a review of the t-test since both methods model mean continuous outcomes.

Recall the previous dataset we used to run a t-test: data from a randomized control trial (RCT) on using indomethacin for prevention of post-ERCP pancreatitis. Below, we select and clean up variables of interest.

```{r data, options}
indo_rct_day3 <- indo_rct %>%
  select(risk, age, pep, psphinc)

indo_rct_day3_clean <- indo_rct_day3 %>%
  mutate(across(c(pep, psphinc),
                ~str_remove(.x, "(.*)_")))
```

It appears that the difference in risk score between patients with and without previous PEP is statistically significant.

```{r raw-t-test, options}
t.test(indo_rct_day3_clean$risk ~ indo_rct_day3_clean$pep, #alternative hypothesis: mean risk differs by pep status
       alternative = "two.sided", #two-sided alternative hypothesis
       mu = 0, #null hypothesis: difference is 0
       paired = FALSE, #group observations are not paired 
       var.equal = FALSE, #variances are not equal
       conf.level = 0.95) #alpha (type-I error) is 0.05
```

# Linear regression

Linear regression, which makes inferences on the mean of the outcome, is more complex but more robust than the previous tests. One drawback of univariate tests measured previously is that we cannot account for confounding variables. A confounding variable is a measure other than the one of interest that can influence our outcome as well as the variable of interest.

For example, let's say that we're interested in studying the association between smoking and lung cancer. Older people are more likely to smoke, and they are also more likely to have cancer in general. If we don't account for age, we may think there's a stronger association between smoking and lung cancer than there actually is.

In linear regression, we fit a line to the data, where the independent variables on the right-hand side of the equation consist of our measure of interest and potential confounders. You will notice that regression looks quite similar to ANOVA, except we can now include continuous variables as independent variables.

In the next section on linear regression, we will go over:

- fitting the model
- visualizing the model (to gain intuition; usually not done)
- interpreting and tidying the results output
- model checks
- predicting outcomes for new data


## Fitting in R

To fit a linear regression in R, we use the `lm` function, which takes in an equation and a dataframe at the minimum.

We can use the `summary` command to get effect estimates and p-values (more detail later).

```{r lm, options}
risk_model <- lm(risk ~ age + pep + psphinc, 
                 data = indo_rct_day3_clean)

summary(risk_model)
```

## Visualizing

Just to build intuition, we can visually see what regression is doing in the case of a univariate analysis (where only our measure of interest is on the right hand side of the equation). In ggplot, there is a `geom_smooth` layer which draws the regression line.

```{r regression-viz, options}
indo_rct_day3_clean %>%
  ggplot(aes(x=age, y=risk)) +
  geom_point() +
  geom_smooth(method="lm")
```

Usually, we don't visualize regressions because each independent variable would add a dimension onto the plot. 

## Interpretation and Tidying Results

To get results from our fit model created from `lm`, we can use the `summary` command. I won't go into detail about interpretation, but each term has two important measures listed: the effect size and p-value. 

```{r results-summary, options}
summary(risk_model)
```

The effect size tells us the estimated effect of a one-unit difference (if independent variable is continuous) or value change (if independent variable is categorical) on the expected value (mean) of the outcome.

$$ risk\ score = 2.099 -0.01 \times age + 0.85 \times I(pep=yes) + 1.05 \times I(psphinc=yes)$$ 

For example, a one-year increase in age is estimated to decrease risk score by about 0.01, and a previous post-ERCP pancreatitis (PEP) is estimated to increase risk score by about 0.854.

The p-value tells us how sure we are about the estimated effect. 

The summary output doesn't look very nice, so we can put it into nice dataframe form using `tidy`. We can make it look even nicer with the kableExtra package below.

```{r results-kable, options}
risk_model %>%
  tidy() 
```

## Model Checks

One important thing to do when fitting linear models is perform checks on the assumptions it makes. If these assumptions are false, then the model may produce misleading results. 

I won't go into details about using these diagnostic plots, but they are easy to generate. The first line here creates a 2x2 plot framework for the multiple plots created by the `plot` function. 

```{r heteroskedastic-residuals, options}
par(mfrow = c(2, 2))
plot(risk_model)
```



## Predicting Outcomes

If we want to predict the expected value of the outcome for observations not in the original dataset, we can use the `predict` function.

```{r results-predict, options}
predict(risk_model, newdata = sample_n(indo_rct_day3_clean, 4))
```

# Logistic Regression

If our outcome is binary (yes/no) and can be coded as 1/0, we cannot use ordinary linear regression, which is meant only for continuous outcomes. 

For example, say we're interested in modelling post-ERCP pancreatitis (`outcome`). We first clean up the dataset with the outcome and chosen covariates, which consist of the treatment arm (placebo vs. indomethacin) and known risk factors associated with post-ERCP pancreatitis.

```{r logistic-data, options}
outcomeDF <- indo_rct %>%
  select(outcome, age, rx, sodsom, paninj, difcan, psphinc, recpanc, pep, sod) %>%
  mutate(across(where(is.factor), 
                ~ str_remove(.x, "_.*") %>%
                  as.numeric))
```

### Fitting in R

We use another base R function, `glm`, which stands for generalized linear model. Like in the normal linear model, we specify the equation and data. We also have an additional parameter `family`, which specifies what sort of generalized linear model we'll be using. The correct family for logistic regression is binomial. We will not go over additional families of linear models in this series.

```{r logistic-fit, options}
outcomeMod <- glm(outcome ~ age + rx + 
                    sodsom + paninj + difcan +
                    psphinc + recpanc + pep + sod, 
                  data = outcomeDF,
                  family = "binomial")

summary(outcomeMod)
```

Like in regular linear regression, we can display the results using the `summary` command and tidy up the table. We interpret the effect estimates as additive effects on the log odds of experiencing the event, $\log(\frac{p}{1-p})$ (where $p$ is the probability of experiencing the event). To get the multiplicative effect on the odds, we exponentiate the displayed effect estimates.

Below is the linear equation used to fit the log odds:

$$\log(\frac{p(PEP)}{1-p(PEP)}) = -1.6 + -0.01 \times age + \dots +  0.1 \times I(sod=Yes)$$

And below is that equation exponentiated so we can fit the odds:

$$\frac{p(PEP)}{1-p(PEP)} = e^{-1.6 + -0.01 \times age + \dots +  0.1 \times I(sod=Yes)}$$

In our example, receiving treatment is estimated to decrease the log odds of PEP by -0.76. This corresponds to a decrease in odds by a multiplicative factor of $e^{-0.76}=0.47$.

```{r logistic-results, options}
outcomeMod %>%
  tidy() %>%
  kbl() %>%
  kable_classic_2()
```

We can also predict binary outcomes using new data points. Below, we predict the probability of a patient with mean values of all covariates getting post-ERCP pancreatitis.  

```{r logistic-predict, options}
predict(outcomeMod,
        newdata = colMeans(outcomeDF %>%
                             select(age, rx, sodsom, 
                                    paninj, difcan, psphinc,
                                    recpanc, pep, sod)) %>%
           t() %>% as.data.frame(),
        type = "response")
```

# Survival

Some research questions have to do with times to first events. Survival models let us find covariates associated with higher hazard of an event (higher probability of experiencing the event for the first time). It allows us to take into account censoring (if a patient is lost to follow up during the study or if the study ends without the patient experiencing an event).

We use a simple dataset `aml` from the `survival` package to illustrate different models we can fit with survival data. This dataset tracks survival of patients with Acute Myelogenous Leukemia, who were either given maintenance chemotherapy or not. The `time` column is either the time of death (if `status` is 1) or time of censoring (if `status` is 0).

```{r aml-data, options}
head(aml)
```

### Visualization

To do work in survival, the model must have for each patient a time and an indicator of whether the time is of an event or censoring. In the `survival` package, we combine these two pieces (usually two dataframe columns) into a `Surv` object.

We can then use this `Surv` object like an outcome. In the below code chunk, we fit a Kaplan-Meier curve, stratified by whether patients get maintenance or not. We then plot the probability of survival at various time points using the Kaplan-Meier curve.


```{r survfit, options}
fit <- survfit(Surv(time=time, event=status) ~ x,
               data=aml)

ggsurvplot(fit) +
  labs(color="Maintenance",
       title = "Survival Probability of AML Patients")
```

### Cox Proportional Hazards Model

Kaplan-Meier curves are used only to describe survival data. For a more formal model from which we can calculate estimated effects of covariates on survival, we can fit a regression model.

In order to use a regression model here, we make the assumption that hazard functions between any two covariate groups are proportional over time. In simple terms, we'd like to see that when we plot survival probabilities stratified by any two group values (as we did using Kaplan-Meier curves), the space between curves doesn't drastically change over time. We can then use a Cox Proportional Hazards model to make statements about, in our example, associations of maintenance chemotherapy with the hazard of death.

```{r cox-ph, options}
coxModel <- coxph(Surv(time, status) ~ x,
      data=aml)

summary(coxModel)
```

### Checking Proportional Hazards Assumption

We can check the proportional hazards assumption using `cox.zph` from the `survival` library. Since the p-values for this test are not significant, we can conclude that there is no evidence that the proportional hazards assumption is violated.

```{r test-ph, options}
cox.zph(coxModel)
```

# Exercises

Consider the `Diabetes` dataset from ROCit package. We will fit linear regression and logistic regression models on cholesterol (`chol`) and diabetes diagnosis (`dtest`) respectively.


```{r diabetes_ex, options}
diabetes_simp_ex <- Diabetes %>%
  select(id, frame, whr, age, chol, dtest) %>%
  drop_na()
```


1.  Fit a linear regression for cholesterol with `frame`, `age`, and `whr` as covariates. Interpret the results.

```{r lm-ex, options}
cholLM <- lm(chol ~ frame + age + whr,
   data = diabetes_simp_ex)

summary(cholLM)
```

2. Make diagnostic plots for the above linear regression.

```{r plot-lm-ex, options}
plot(cholLM)
```

3. Predict the cholesterol of a patient with a "small" frame, an age of 30, and `whr` of 0.8.

```{r predict-lm-ex, options}
predict(cholLM, 
        newdata = data.frame(frame="small",
                             age = 30,
                             whr = 0.8))
```

4. Before you fit a logistic regression for `dtest`, we need to make sure it is a binary (0/1) variable. Uncomment and fill in the below code, which uses the `mutate` function to edit the `dtest` column.

```{r binarize-ex, options}
diabetes_simp_ex_bin <- diabetes_simp_ex %>%
     mutate(dtest = if_else(dtest=="+",
                            1,
                            0))
```

5. Fit a logistic regression for diabetes status with `frame`, `age`, `whr`, and `chol` as covariates. Interpret the results.

```{r logistic-ex, options}
diabGLM <- glm(dtest ~ frame + age + whr + chol,
    data = diabetes_simp_ex_bin,
    family = "binomial")

summary(diabGLM)
```
